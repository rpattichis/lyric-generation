{"cells":[{"cell_type":"code","source":["# TODO: Locate the root of the directory\n","BASE_DIR = '/content/drive/MyDrive/cs260-final-project'"],"metadata":{"id":"GhPbvx8BtZrv"},"id":"GhPbvx8BtZrv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3dbc2bbd","metadata":{"id":"3dbc2bbd"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs260/assignments/assignment6/'\n","FOLDERNAME = 'src/vanilla-rnn'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('{}/{}'.format(BASE_DIR, FOLDERNAME))"]},{"cell_type":"markdown","id":"35899740","metadata":{"tags":["pdf-title"],"id":"35899740"},"source":["# Song Generation with RNNs\n","Import all of the necessary functions to use here."]},{"cell_type":"code","execution_count":null,"id":"211b4cef","metadata":{"tags":["pdf-ignore"],"id":"211b4cef"},"outputs":[],"source":["# Setup cell.\n","import time, os, json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import re\n","\n","from cs260.rnn_layers import *\n","from cs260.lyric_solver import CaptioningSolver\n","from cs260.classifiers.rnn import CaptioningRNN\n","from cs260.lyric_utils import load_lyric_data, decode_captions, sample_lyric_minibatch, write_lyric_data\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","id":"74855464","metadata":{"tags":["pdf-ignore"],"id":"74855464"},"source":["# Lyric Dataset\n","\n","**Features.** Includes the Artist and the Topic\n","\n","**Lyrics.** Includes the tokenized version of the lyrics\n","\n","**Tokens.** There are a couple special tokens that we add to the vocabulary, and we have taken care of all implementation details around special tokens for you. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each lyric respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing lyrics of different lengths, we pad short lyrics with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens."]},{"cell_type":"markdown","source":["**NOTE:** For first-time users, run all three cells and ensure the following cells are working. This creates an h5 file so that our training data will be ready to be fed to the RNN."],"metadata":{"id":"IAibOZpNtE7-"},"id":"IAibOZpNtE7-"},{"cell_type":"code","source":["num_topics = 10"],"metadata":{"id":"EYY90Jk6aJML"},"id":"EYY90Jk6aJML","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_file = f'data/train/big-lda-train-{num_topics}.csv'\n","val_file = f'data/val/big-lda-val-{num_topics}.csv'\n","h5_file = f'data/h5/big-lda-{num_topics}.h5'\n","write_lyric_data(train_file, val_file, h5_file, base_dir=BASE_DIR)"],"metadata":{"id":"5SjQIL1fD8DJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670890032877,"user_tz":480,"elapsed":58253,"user":{"displayName":"Sparclight","userId":"18088977216357673987"}},"outputId":"5338eb02-8971-4da0-faa4-3b02c05d24be"},"id":"5SjQIL1fD8DJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["base dir  /content/drive/MyDrive/cs260-final-project\n","(1/6) Extracting training data...\n","(2/6) Extracting validation data...\n","(3/6) Extracting tokenizer...\n","(4/6) Tokenizing training lyrics...\n","(5/6) Tokenizing validation lyrics...\n","(6/6) Storing data into h5 file data/h5/big-lda-10.h5...\n","Done!\n"]}]},{"cell_type":"code","source":["data = load_lyric_data(10, base_dir=BASE_DIR) # num_topics, base_dir, max_train=None\n","\n","# Print out all the keys and values from the data dictionary.\n","for k, v in data.items():\n","    if type(v) == np.ndarray:\n","        print(k, type(v), v.shape, v.dtype)\n","    else:\n","        print(k, type(v), len(v))"],"metadata":{"id":"1slI6YaiiYBc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670890058185,"user_tz":480,"elapsed":24052,"user":{"displayName":"Sparclight","userId":"18088977216357673987"}},"outputId":"534e1178-306c-414f-f300-eb773a953688"},"id":"1slI6YaiiYBc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["base dir  /content/drive/MyDrive/cs260-final-project\n","train_artist <class 'numpy.ndarray'> (178712,) object\n","train_topic_id <class 'numpy.ndarray'> (178712,) int64\n","train_lyric <class 'numpy.ndarray'> (178712, 512) int64\n","val_artist <class 'numpy.ndarray'> (26254,) object\n","val_topic_id <class 'numpy.ndarray'> (26254,) int64\n","val_lyric <class 'numpy.ndarray'> (26254, 512) int64\n","idx_to_word <class 'list'> 241489\n","word_to_index <class 'dict'> 241489\n","idx_to_artist <class 'list'> 6921\n","artist_to_index <class 'dict'> 6921\n","train_features <class 'numpy.ndarray'> (178712, 1001) float64\n","val_features <class 'numpy.ndarray'> (26254, 1001) float64\n"]}]},{"cell_type":"markdown","metadata":{"id":"1007806b"},"source":["# Lyric RNN Model on Small Data\n","Run this cell to fit the lyric data to the vanilla RNN model. \n","\n","**WARNING:** Running this cell take up all of your RAM depending on your Colab plan."],"id":"1007806b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5a741fe0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a403b8fe-7b44-40b0-b949-d1235ea59fae"},"outputs":[{"output_type":"stream","name":"stdout","text":["base dir  /content/drive/MyDrive/cs260-final-project\n","(Iteration 1 / 50) training loss: 3815.593388, validation loss: 940.276811\n"]}],"source":["np.random.seed(260)\n","\n","small_data = load_lyric_data(10, base_dir=BASE_DIR, max_train=50)\n","\n","small_rnn_model = CaptioningRNN(\n","    cell_type='rnn',\n","    word_to_idx=small_data['word_to_index'],\n","    input_dim=small_data['train_features'].shape[1],\n","    hidden_dim=512,\n","    wordvec_dim=256,\n",")\n","\n","small_rnn_solver = CaptioningSolver(\n","    small_rnn_model, small_data, # see how the batch is constructed\n","    update_rule='adam',\n","    num_epochs=1, #50\n","    batch_size=1, #25\n","    val_batch_size=5,\n","    optim_config={\n","     'learning_rate': 5e-3,\n","    },\n","    lr_decay=0.95,\n","    verbose=True, print_every=10,\n",")\n","\n","small_rnn_solver.train()\n","\n","# Plot the training losses.\n","plt.plot(small_rnn_solver.loss_history)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training loss history')\n","plt.show()"],"id":"5a741fe0"},{"cell_type":"code","execution_count":null,"metadata":{"test":"rnn_final_training_loss","id":"14aa595d"},"outputs":[],"source":["print('Final loss: ', small_rnn_solver.loss_history[-1])"],"id":"14aa595d"},{"cell_type":"markdown","metadata":{"id":"3f7ad3f3"},"source":["# RNN Sampling at Test Time\n","Unlike classification models, image captioning models behave very differently at training time vs. at test time. At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep. At test time, we sample from the distribution over the vocabulary at each timestep and feed the sample as input to the RNN at the next timestep.\n","\n","In the file `cs260/classifiers/rnn.py`, implement the `sample` method for test-time sampling. After doing so, run the following to sample from your overfitted model on both training and validation data. The samples on training data should be very good. The samples on validation data, however, probably won't make sense."],"id":"3f7ad3f3"},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"f7f88353"},"outputs":[],"source":["# sample the lyric and see if the model generates the same lyric!\n","for split in ['train', 'val']:\n","    minibatch = sample_lyric_minibatch(small_data, split=split, batch_size=2)\n","    gt_lyrics, features = minibatch\n","    gt_lyrics = decode_captions(gt_lyrics, data['idx_to_word'])\n","\n","    sample_lyrics = small_rnn_model.sample(features)\n","    sample_lyrics = decode_captions(sample_lyrics, data['idx_to_word'])\n","\n","    for gt_lyric, sample_lyric in zip(gt_lyrics, sample_lyrics):    \n","        plt.title('%s\\n%s\\nGT:%s' % (split, sample_lyric, gt_lyric))\n","        plt.axis('off')\n","        plt.show()"],"id":"f7f88353"}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}